---
title: "Master Project Proper"
output: html_document
date: "2024-03-01"
---


Where betas are hyperparameters fit to the data

```{r packages, include=TRUE, echo=TRUE, eval=TRUE, cache=FALSE}

library("matrixStats")
library(actuar)

library(LaplacesDemon)
#library(Rfast)
library(mnormt)
library(matrixStats)

library(stats)
library(statmod)

library(xtable)
library(knitr)

if(!require(golazo)){
    library(devtools)
    install_github("pzwiernik/golazo", build_vignettes=FALSE)
}
library(golazo)

library(BDgraph)
library(ssgraph)

#install.packages(lubridate)
library(lubridate)
#install.packages('rstan')
library(rstan)
```


```{r}
stocks <- read.csv('/Users/harrymorley/Desktop/Work/BSE Work/Master Project/Dropbox/Data/StockReturns.csv') #replace this with stock data #path

train_year <- 2015

test_year <- 2016

stocks$Year <- year(stocks$Date)

years <- c(train_year, test_year)

preprocessing <- function(stocks, years) {

stocks_clean_subset <- stocks[stocks$Year %in% years,] #subset whole dataset to find only training and testing years

stocks_clean <- stocks_clean_subset[, !apply(stocks_clean_subset, 2, function(x) any(is.na(x)))] #remove NAs from these years

return(stocks_clean)
}

stocks_final <- preprocessing(stocks, years)


train_set <- function(stocks_final) {
  
stocks_train <- stocks_final[stocks_final$Year == train_year,] #divide further into specific year

stocks_train <- stocks_train[, 3:ncol(stocks_train)] #remove date and index

stocks_train$Year <- NULL

stocks_train <- data.frame(sapply(stocks_train, function(x) as.numeric(as.character(x)))) #make numeric

return(stocks_train)
  
}

test_set <- function(stocks_final) {
  
stocks_test <- stocks_final[stocks_final$Year == test_year,] #divide further into specific year

stocks_test <- stocks_test[, 3:ncol(stocks_test)] #remove date and index

stocks_test$Year <- NULL

stocks_test <- data.frame(sapply(stocks_test, function(x) as.numeric(as.character(x)))) #make numeric

return(stocks_test)
  
}

stocks_train <- train_set(stocks_final)/100

stocks_test <- test_set(stocks_final)/100

stocks_train_matrix <- as.matrix(stocks_train)

stocks_test_matrix <- as.matrix(stocks_test)
```



Now we standardise the data: 

```{r}
train_scaled <- scale(stocks_train) 

train_mean <- attr(train_scaled, "scaled:center")
train_sd <- attr(train_scaled, "scaled:scale")

test_scaled <- scale(stocks_test, center = train_mean, scale = train_sd)
```

```{r}
adjacency_matrix <- read.csv('/Users/harrymorley/Desktop/Work/BSE Work/Master Project/Dropbox/Data/Rafa Networks/Sym_adj_mat_2015_USE.csv') #replace this with path to sector #matrix

# Set row names from the first column and then remove the first column
rownames(adjacency_matrix) <- adjacency_matrix[, 1]
adjacency_matrix <- adjacency_matrix[, -1]  # Remove the first column which is now set as row names

# Assuming stocks_train is already loaded and has correct column names
# Identify columns present in both adjacency_matrix and stocks_train
common_cols <- colnames(adjacency_matrix) %in% colnames(stocks_train)

# Filter columns to those that are common
adjacency_matrix <- adjacency_matrix[, common_cols]

# Ensure that row names are aligned with the new column structure
# Filter rows to ensure they match the new column names
if (!all(rownames(adjacency_matrix) %in% colnames(adjacency_matrix))) {
    adjacency_matrix <- adjacency_matrix[rownames(adjacency_matrix) %in% colnames(adjacency_matrix), ]
}

# Check if the adjacency matrix is square
if (nrow(adjacency_matrix) == ncol(adjacency_matrix)) {
    print("The adjacency matrix is square and correctly filtered.")
} else {
    print("There was an error in filtering the adjacency matrix.")
}

# Check the alignment of row and column names
if (identical(rownames(adjacency_matrix), colnames(adjacency_matrix))) {
    print("Row and column names are aligned.")
} else {
    print("Mismatch in row and column names alignment.")
}

adjacency_matrix_2 <- as.matrix(adjacency_matrix)
```

```{r}

adjacency_matrix_1 <- adjacency_matrix

adjacency_matrix <- read.csv('/Users/harrymorley/Desktop/Work/BSE Work/Master Project/Network Code/adjacency_matrix.csv') #replace this #with path to your matrix

# Set row names from the first column and then remove the first column
rownames(adjacency_matrix) <- adjacency_matrix[, 1]
adjacency_matrix <- adjacency_matrix[, -1]  # Remove the first column which is now set as row names

# Assuming stocks_train is already loaded and has correct column names
# Identify columns present in both adjacency_matrix and stocks_train
common_cols <- colnames(adjacency_matrix) %in% colnames(stocks_train)

# Filter columns to those that are common
adjacency_matrix <- adjacency_matrix[, common_cols]

# Ensure that row names are aligned with the new column structure
# Filter rows to ensure they match the new column names
if (!all(rownames(adjacency_matrix) %in% colnames(adjacency_matrix))) {
    adjacency_matrix <- adjacency_matrix[rownames(adjacency_matrix) %in% colnames(adjacency_matrix), ]
}

# Check if the adjacency matrix is square
if (nrow(adjacency_matrix) == ncol(adjacency_matrix)) {
    print("The adjacency matrix is square and correctly filtered.")
} else {
    print("There was an error in filtering the adjacency matrix.")
}

# Check the alignment of row and column names
if (identical(rownames(adjacency_matrix), colnames(adjacency_matrix))) {
    print("Row and column names are aligned.")
} else {
    print("Mismatch in row and column names alignment.")
}

adjacency_matrix_2 <- as.matrix(adjacency_matrix)

```

```{r}
#defining priors:

lambda_0 <- 0.1

omega_jj <- seq(0, ncol(train_scaled), length.out = 1000)
plot(omega_jj, dexp(omega_jj, rate = lambda_0), xlab = "omega_jj", ylab = "Density", type = "l", lwd = 3)

#off-diagonals
lambda_0_set <- 0.1

#Bayesian_GLASSO_hyperprior_summaries_lambda0Fixed(p = ncol(train_scaled), a_1_0 = 15, b_1_0 = 1, lambda_0 = lambda_0_set, N_MC = 100, thresh = 0.01) 

a_0 <- 4
b_0 <- 2 * sqrt(2)## somehow max variance 

lambda_1_seq <- seq(0, ncol(train_scaled), length.out = 1000)
plot(lambda_1_seq, dgamma(lambda_1_seq, shape = a_0, rate = b_0), xlab = "lambda_1", ylab = "Density", type = "l", lwd = 3)

a_0_set <- 4
b_0_set <- 2 * sqrt(2)
```



```{r}
#using full lambda + both networks

#epsilon_0 <- 0.2 good performing parameters (all in 20%s)
#epsilon_1 <- 0.01
#epsilon_2 <- 0.5


#These control the step size in each proposal. 
epsilon_0 <- 0.1
epsilon_1 <- 0.1
epsilon_2 <- 0.2

prior_params_p20 <- list("a_1_0" = a_0_set,
                        "b_1_0" = b_0_set,
                        "lambda_0" = lambda_0_set)

N <- 10000# MCMC iterations

p <- 30

catrate <- 20 #how often to print iteration. Larger is more, use powers of ten

set.seed(13)

#initalisation
Omega_init <- matrix(0, nrow = p, ncol = p) + diag(1, p)
tau_init <- matrix(1, nrow = p, ncol = p)
mu_beta0 <- 0
mu_beta1 <- 0
mu_beta2 <- 0

beta_var_init <- sqrt(log(2))

stocks_train_matrix <- as.matrix(train_scaled)

n_obs <- nrow(stocks_train_matrix)

S <- t(stocks_train_matrix[,1:p])%*%stocks_train_matrix[,1:p]

A_1 <- 1

random_matrix_1 <- matrix(runif(p * p), nrow = p, ncol = p)

# Make the matrix symmetric
symmetric_matrix_1 <- (random_matrix_1 + t(random_matrix_1)) / 2

diag(symmetric_matrix_1) <- 0

random_matrix_2 <- matrix(runif(p * p), nrow = p, ncol = p)

# Make the matrix symmetric
symmetric_matrix_2 <- (random_matrix_2 + t(random_matrix_2)) / 2

diag(symmetric_matrix_2) <- 0



A_2 <- as.matrix(adjacency_matrix_2[1:p,1:p])

A_2[abs(A_2) < 0.1] <- 0

A_2[abs(A_2) != 0] <- 1

A_3 <- symmetric_matrix_1#as.matrix(adjacency_matrix_1[1:p,1:p])

#all_beta <- all_beta_func(N = N, S = S, a_1_0 = prior_params_p20$a_1_0, b_1_0 = prior_params_p20$b_1_0, lambda_0 = prior_params_p20$lambda_0, n_obs = n_obs, Omega_init = Omega_init, tau_init = tau_init, beta_var_init = beta_var_init, mu_beta0 = mu_beta0, mu_beta1 = mu_beta1, mu_beta2 = mu_beta2, epsilon_0, epsilon_1, epsilon_2, A_1, A_2, catrate)


```

```{r}
#For all_beta function diagnostics

burnin <- 1000

lambda_1_full <- all_beta$lambda_1_chain[burnin:N]

beta0_full <- all_beta$beta0_chain[burnin:N]

beta1_full <- all_beta$beta1_chain[burnin:N]

beta2_full <- all_beta$beta2_chain[burnin:N]

#plot(lambda_1_full, type = 'l', col = 1)

plot(beta0_full, type = 'l', col = 1)

plot(beta1_full, type = 'l', col = 2)

plot(beta2_full, type = 'l', col = 3)

#points(lambda_1_test, type = 'l', col = 2)

cat('Mean Lambda:', mean(lambda_1_full), '  ')

#plot(density(lambda_1_full), type = 'l', col = 1)

plot(density(beta0_full), type = 'l', col = 3)

plot(density(beta1_full), type = 'l', col = 2)

plot(density(beta2_full), type = 'l', col = 4)

#points(density(lambda_1_test), type = 'l', col = 2)

cat('beta_0 acc rate:', mean(all_beta$accrate_0), '  ')

cat('beta_1 acc rate:', mean(all_beta$accrate_1), '  ')

cat('beta_2 acc rate:', mean(all_beta$accrate_2), '  ')

cat('beta_0 value:', mean(beta0_full), '  ')

cat('beta_1 value:', mean(beta1_full), '  ') 

cat('beta_2 value:', mean(beta2_full), '  ') 

acf(beta0_full)

acf(beta1_full)

acf(beta2_full)
```

```{r}
gen_beta <- gen_network_func(N = N, S = S, a_1_0 = prior_params_p20$a_1_0, b_1_0 = prior_params_p20$b_1_0, lambda_0 = prior_params_p20$lambda_0, n_obs = n_obs, Omega_init = Omega_init, tau_init = tau_init, beta_initials = c(0,0), epsilons = c(0.3,0.2), A = list(A_2), catrate = catrate, mu_beta_0 = 0, mu_beta_1 = 1, var_beta = beta_var_init)




#c(0.12,0.01,0.4)
```

Put in one network each time with:

no network
raf network original 
raf network with ones and zeros
rafa network original / 100 

Do these with 20 stocks, then run with all stocks for no network and with the network that made the most sense. Have some plots of the matrices. Partial correlation (values between zero and one - don't use precision as scale can be misleading) plots of no network and network, hopefully they are different. Provide the plot with GLASSO against values in the network, one we saw together last time so we can show Jack the relationship and see if the plot supports inclusion of network. 

Do this for a couple of years for tomorrow. All the stocks preferable. First do with a few so we have something to see, then do it 
with all the stocks. 


This is rafa network with ones and zeros:

Metropolis Hastings not needed for no network as a benchmark. We have generalised the GLASSO (Nick and Jack have) so it is slightly different.

2008-2009, 2014-2017, 2020-2021. Create plot for every year, then say 'we did this plot and focussed on the five years that looked the most promising'. Eyeball the plots and see if there is any dependence. 

There a function in R and look at the effective sample size which tells you 'if I were to swap this set of samples for a set of uncorrelated sample size, how many would be in my sample'. Jack has used one from Laplace's demon. You want that to be at least 100. 

Prepare code for a couple of years and send to Nico. 

We need the partial correlation of the omega with and without the networks. The precision network with and without the network. Make heatmaps for omega before and after adding the network. Can also do things like 'count the number of zeros', 'count number of edges', does the network allow for a sparser network. If number less than 0.01. Partial correlation before and after the network equals one. 

If the 0.5 - A looks good, try using it on the data - the transformed network. In the thesis, have all of them, but justification for this in terms of the plot, big values in this network are connected with strong correlations in the data, so we are reparameterising the network to make it look more true. 

Run both Raf's and the other one separately, and then you can run them together. Try to be organised about what you're going to run. It might be worth focussing on five years, and running them individually, and then maybe doing more if you have time. We can ask 'if it is true that not trading is just the same as being in the same network, does adding the sector matrix work?'

We have a different penalisation for the diagonal which is different from the Wang method, this is because David has a paper saying not to do this. 

Prior elicitation normally isnt that important, but this time we have big p, smallish n, plus positive definite indicator function going on, so it does matter in this case at least to an extent. 

Heatmap yoy change for omega, network. 

```{r}
burnin <- 2000

lambda_1_full <- gen_beta$lambda_1_chain[burnin:N]

beta0_full <- gen_beta$beta_chain[1, burnin:N]

beta1_full <- gen_beta$beta_chain[2, burnin:N]

#beta2_full <- gen_beta$beta_chain[3, burnin:N]

#plot(lambda_1_full, type = 'l', col = 1)

plot(beta0_full, type = 'l', col = 1)

plot(beta1_full, type = 'l', col = 2)

#plot(beta2_full, type = 'l', col = 3)

#points(lambda_1_test, type = 'l', col = 2)

cat('Mean Lambda:', mean(lambda_1_full), '  ')

#plot(density(lambda_1_full), type = 'l', col = 1)

plot(density(beta0_full), type = 'l', col = 3)

plot(density(beta1_full), type = 'l', col = 2)

#plot(density(beta2_full), type = 'l', col = 4)

#points(density(lambda_1_test), type = 'l', col = 2)

cat('beta_0 acc rate:', mean(gen_beta$accrate[1,]), '  ')

cat('beta_1 acc rate:', mean(gen_beta$accrate[2,]), '  ')

#cat('beta_2 acc rate:', mean(gen_beta$accrate[3,]), '  ')

cat('beta_0 value:', mean(beta0_full), '  ')

cat('beta_1 value:', mean(beta1_full), '  ') 

#cat('beta_2 value:', mean(beta2_full), '  ') 

acf(beta0_full)

acf(beta1_full)

#acf(beta2_full)
```

```{r}

N <- 300

raf_normalised <- adjacency_matrix_2

raf_normalised <- raf_normalised/100

raf_normalised <- as.matrix(raf_normalised[1:p,1:p])

raf_normalised <- abs(0.5 - raf_normalised)

start_time <- Sys.time()

raf_normalised <- gen_network_func(N = N, S = S, a_1_0 = prior_params_p20$a_1_0, b_1_0 = prior_params_p20$b_1_0, lambda_0 = prior_params_p20$lambda_0, n_obs = n_obs, Omega_init = Omega_init, tau_init = tau_init, beta_initials = c(0,0), epsilons = c(0.01,0.02), A = list(raf_normalised), catrate = catrate, mu_beta  <- 0, var_beta = beta_var_init)

end_time <- Sys.time()

elapsed_time <- end_time - start_time

#c(0.12,0.01,0.4)
```


```{r}
burnin <- 100

lambda_1_full <- raf_normalised$lambda_1_chain[burnin:N]

beta0_full <- raf_normalised$beta_chain[1, burnin:N]

beta1_full <- raf_normalised$beta_chain[2, burnin:N]

#beta2_full <- gen_beta$beta_chain[3, burnin:N]

#plot(lambda_1_full, type = 'l', col = 1)

plot(beta0_full, type = 'l', col = 1)

plot(beta1_full, type = 'l', col = 2)

#plot(beta2_full, type = 'l', col = 3)

#points(lambda_1_test, type = 'l', col = 2)

cat('Mean Lambda:', mean(lambda_1_full), '  ')

#plot(density(lambda_1_full), type = 'l', col = 1)

plot(density(beta0_full), type = 'l', col = 3)

plot(density(beta1_full), type = 'l', col = 2)

#plot(density(beta2_full), type = 'l', col = 4)

#points(density(lambda_1_test), type = 'l', col = 2)

cat('beta_0 acc rate:', mean(raf_normalised$accrate[1,]), '  ')

cat('beta_1 acc rate:', mean(raf_normalised$accrate[2,]), '  ')

#cat('beta_2 acc rate:', mean(gen_beta$accrate[3,]), '  ')

cat('beta_0 value:', mean(beta0_full), '  ')

cat('beta_1 value:', mean(beta1_full), '  ') 

#cat('beta_2 value:', mean(beta2_full), '  ') 

acf(beta0_full)

acf(beta1_full)

#acf(beta2_full)
```




```{r}
Omega_mean_scaled <- apply(all_beta$Omega_chain[(200 + 1):N, , ], c(2, 3), mean)
```





